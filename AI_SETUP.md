# ðŸ¤– Local AI Integration Setup Guide

## Overview

This project uses **Llama 3.2 (3B)** via **Ollama** for local AI features:
- âœ… Smart task duration estimation
- âœ… Automatic dependency detection
- âœ… Task categorization
- âœ… 100% private (runs locally)
- âœ… Zero API costs
- âœ… Fast (<500ms response time)

---

## ðŸš€ Quick Start

### Step 1: Install Ollama

**macOS:**
```bash
brew install ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Windows:**
Download from https://ollama.com/download

### Step 2: Start Ollama Service

```bash
# Start Ollama in the background
ollama serve
```

Keep this terminal running, or run it as a background service.

### Step 3: Pull Llama 3.2 Model

```bash
# Download the 3B parameter model (~2GB)
ollama pull llama3.2:3b
```

**Alternative models:**
```bash
# Faster but less accurate (1.5GB)
ollama pull llama3.2:1b

# More accurate but slower (4.7GB)
ollama pull llama3.2:7b

# Microsoft's Phi-3 (good alternative)
ollama pull phi3:mini
```

### Step 4: Install Backend Dependencies

```bash
cd backend
pip install -r requirements.txt
```

### Step 5: Test AI Service

```bash
# Test Ollama directly
ollama run llama3.2:3b "Estimate duration for task: Design database schema"

# Should respond with something like:
# "Based on typical software development timelines, designing a database 
#  schema typically takes 2-3 days..."
```

---

## ðŸ§ª Testing the Integration

### 1. Start Backend
```bash
cd backend
python main.py
```

### 2. Test AI Health Endpoint
```bash
curl http://localhost:8000/api/ai/health
```

Expected response:
```json
{
  "status": "healthy",
  "model": "llama3.2:3b",
  "provider": "Ollama (Local)"
}
```

### 3. Test Duration Estimation
```bash
curl -X POST http://localhost:8000/api/ai/estimate-duration \
  -H "Content-Type: application/json" \
  -d '{"task_name": "Design database schema", "task_type": "design"}'
```

Expected response:
```json
{
  "days": 2.5,
  "confidence": 85,
  "reasoning": "Database schema design typically requires 2-3 days for planning, modeling, and review"
}
```

### 4. Test Dependency Detection
```bash
curl -X POST http://localhost:8000/api/ai/detect-dependencies \
  -H "Content-Type: application/json" \
  -d '{
    "tasks": [
      {"id": "1", "name": "Design database schema"},
      {"id": "2", "name": "Implement API endpoints"},
      {"id": "3", "name": "Write unit tests"}
    ]
  }'
```

---

## ðŸŽ¨ Using AI Features in the UI

### In Task Edit Dialog

1. Enter a task name (e.g., "Create user authentication system")
2. Click the **"ðŸ¤– AI Suggest"** button
3. AI will provide:
   - Estimated duration (in days)
   - Confidence score
   - Task category
   - Reasoning

4. Click **"âœ“ Apply Duration"** to use the suggestion

### AI Suggestions Panel

The AI helper shows:
- **Duration Estimate**: How long the task should take
- **Confidence Score**: How certain the AI is (color-coded)
- **Category**: Task type (design, development, testing, etc.)
- **Reasoning**: Why the AI made this estimate

---

## âš™ï¸ Configuration

### Change AI Model

Edit `backend/ai_service.py`:
```python
class LocalAIService:
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.model = "llama3.2:3b"  # Change this
```

**Recommended models:**
- `llama3.2:1b` - Fastest (1.5GB)
- `llama3.2:3b` - Balanced (2GB) â­ **Recommended**
- `llama3.2:7b` - Most accurate (4.7GB)
- `phi3:mini` - Microsoft alternative (3.8GB)
- `mistral:7b` - Good for complex reasoning (4.1GB)

### Adjust AI Temperature

Lower temperature = more consistent, higher = more creative

```python
"options": {
    "temperature": 0.3,  # 0.0 = deterministic, 1.0 = creative
    "top_p": 0.9,
}
```

---

## ðŸ› Troubleshooting

### "AI service unavailable"

**Check if Ollama is running:**
```bash
curl http://localhost:11434/api/tags
```

If not running:
```bash
ollama serve
```

### "Model not found"

**Pull the model:**
```bash
ollama pull llama3.2:3b
```

### Slow responses

**Try a smaller model:**
```bash
ollama pull llama3.2:1b
```

Or increase timeout in `ai_service.py`:
```python
async with httpx.AsyncClient(timeout=60.0) as client:  # Increase from 30
```

### High CPU usage

**Use GPU acceleration (if available):**
Ollama automatically uses GPU if available (CUDA, Metal, ROCm)

Check GPU usage:
```bash
ollama ps
```

---

## ðŸ“Š Performance Benchmarks

**Llama 3.2 3B on M1 Mac:**
- Duration estimation: ~300-500ms
- Dependency detection: ~800ms-1.2s
- Task categorization: ~200-400ms

**Llama 3.2 3B on Intel i7:**
- Duration estimation: ~600-900ms
- Dependency detection: ~1.5-2s
- Task categorization: ~400-600ms

---

## ðŸ”’ Privacy & Security

âœ… **100% Local** - No data sent to external servers
âœ… **No API Keys** - No cloud service required
âœ… **Offline Capable** - Works without internet
âœ… **Private** - Your project data stays on your machine

---

## ðŸ’¡ Tips for Best Results

1. **Use descriptive task names**: "Design user authentication flow" is better than "Auth"
2. **Be specific**: Include context like "frontend", "backend", "API", etc.
3. **Review suggestions**: AI is a helper, not a replacement for judgment
4. **Adjust confidence threshold**: Ignore suggestions below 60% confidence

---

## ðŸš€ Next Steps

Once AI is working, you can:
1. Add AI suggestions to the task creation form
2. Enable auto-categorization on task import
3. Add bulk dependency detection for entire project
4. Create AI-powered project timeline optimization

---

## ðŸ“š Resources

- Ollama Documentation: https://ollama.com/docs
- Llama 3.2 Model Card: https://ollama.com/library/llama3.2
- FastAPI Async: https://fastapi.tiangolo.com/async/

